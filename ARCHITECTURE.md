# Architecture

This document explains how PolicyPulse actually works—component by component, design decisions, and failure modes we encountered during development.

## System Overview

PolicyPulse is a retrieval system with four layers:

**1. Storage Layer**
- ChromaDB (SQLite-backed vector store) at `./chromadb_data/`
- Raw policy data in `Data/` (CSVs and text files)
- Logs written to console (no persistent log files currently)

**2. Processing Layer**
- Sentence-transformers for embedding generation (384-dim vectors)
- Policy detection via keyword matching
- Language detection via `langdetect`

**3. Reasoning Layer**
- Vector similarity search (cosine distance)
- Time-decay weighting for recency
- Template-based answer synthesis

**4. Interface Layer**
- FastAPI REST API (8 endpoints)
- Streamlit web UI (5 tabs)

## Data Flow

### Query Processing (Step-by-Step)

**1. User submits query**
```
Input: "What is NREGA wage rate?"
```

**2. Policy detection**
```python
# Keyword matching against policy terms
keywords = ["nrega", "mgnrega", "rural employment", "100 days"]
if any(k in query.lower() for k in keywords):
    policy_id = "NREGA"
else:
    policy_id = "NREGA"  # Default fallback
```

This is intentionally simple. We tried semantic policy detection (embed query, find nearest policy centroid) but keyword matching worked better in testing for common queries.

**3. Language detection**
```python
# langdetect library
detected_lang = detect(query)  # "en", "hi", "ta", etc.
if detected_lang != "en":
    query_english = translate(query, src=detected_lang, dest="en")
```

Fallback: if detection fails, assume English.

**4. Embedding generation**
```python
# sentence-transformers all-MiniLM-L6-v2
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')
query_embedding = model.encode(query_english)  # 384-dim vector
```

This runs on CPU, takes ~50ms per query.

**5. Vector search**
```python
# ChromaDB query
results = collection.query(
    query_embeddings=[query_embedding],
    n_results=5,
    where={"policy_id": "NREGA"}  # Filter by detected policy
)
```

Returns: `ids`, `documents`, `metadatas`, `distances`

**6. Time-decay weighting**
```python
# Exponential decay based on age
current_year = 2026
for result in results:
    age_years = current_year - int(result['metadata']['year'])
    decay_weight = exp(-0.1 * age_years)
    adjusted_score = (1 - result['distance']) * decay_weight
```

Example: 2020 data gets weight 0.55, 2024 data gets weight 0.96.

**7. Answer synthesis**
```python
# Template-based (not LLM)
answer_parts = []
for result in top_3_results:
    answer_parts.append(
        f"{result['policy_id']} ({result['year']}): {result['document']}"
    )
final_answer = "\n\n".join(answer_parts)
```

**8. Translation (optional)**
If user requested output in Hindi, translate `final_answer` back.

**9. TTS (optional)**
If audio output requested, generate MP3 via gTTS.

### Total pipeline latency breakdown (measured)
- Policy detection: <5ms
- Language detection: ~20ms
- Embedding: ~50ms
- Vector search: ~80ms
- Time-decay ranking: ~10ms
- Answer synthesis: ~15ms
- **Total: ~180ms** (without translation/TTS)

---

## Component Details

### ChromaDB Setup (`src/chromadb_setup.py`)

**Why ChromaDB?**
- Pure Python, no Docker, no separate server process
- Persists to local directory (SQLite backend)
- Free, MIT license

**Alternatives we considered:**
- Qdrant: requires Docker or separate server, overkill for hackathon
- FAISS: in-memory only, loses data on restart
- Pinecone: cloud service, requires API key and internet

**Collection schema:**
```python
{
    "id": str,  # Unique chunk ID
    "embedding": List[float],  # 384-dim vector (auto-generated by ChromaDB)
    "document": str,  # Text content
    "metadata": {
        "policy_id": str,  # "NREGA", "RTI", etc.
        "year": str,  # "2020", "2021", etc.
        "modality": str,  # "temporal", "budget", "news"
        "sentiment": str,  # "positive", "neutral", "negative"
        "decay_weight": float,  # Time-decay multiplier
        "access_count": int  # How many times retrieved
    }
}
```

**Failure mode we hit:**
Twice during development, ChromaDB's SQLite backend got corrupted (error: `no such column: collections.topic`). Cause: we were modifying collection schema while ingesting data. Fix: added `fix_chromadb.bat` script that deletes `chromadb_data/` and re-ingests.

For production, would need:
- Regular backups
- Schema migration strategy
- Possibly move to distributed vector DB

**Current limits:**
- ~50k documents before performance degrades noticeably (we have 847 chunks)
- Single-threaded queries (ChromaDB doesn't support concurrent writes well)
- No horizontal scaling

---

### Reasoning Engine (`src/reasoning.py`)

**Purpose:** Convert vector search results into structured answers.

**Process:**
1. Receive ChromaDB results (list of dicts with `id`, `document`, `metadata`, `distance`)
2. Filter out low-confidence results (distance > 0.6 ~ similarity < 0.4)
3. Rank by adjusted score (similarity × time-decay weight)
4. Group by modality (temporal, budget, news)
5. Format as structured text

**Confidence scoring:**
```python
# Average similarity of top-3 results
avg_similarity = mean([1 - d for d in top_3_distances])

if avg_similarity >= 0.70:
    confidence = "High"
elif avg_similarity >= 0.45:
    confidence = "Medium"
else:
    confidence = "Low"
```

**Why not use an LLM here?**
We tried Gemini integration for answer synthesis. Problems:
1. Added 1-2 seconds latency (unacceptable for query loop)
2. Occasionally hallucinated facts ("NREGA was amended in 2017" when no 2017 data exists)
3. Made evaluation harder—how do you validate generated text?

Retrieval-only is:
- Fast (180ms)
- Reproducible (same query → same answer)
- Auditable (can trace every fact to source chunk)

Government use case prioritizes correctness over fluency.

---

### Drift Analysis (`src/drift.py`)

**Purpose:** Quantify how much a policy has changed semantically over time.

**Algorithm:**
1. Retrieve all chunks for a policy, grouped by year
2. For each year, compute centroid embedding (mean of all chunk embeddings for that year)
3. Calculate cosine similarity between consecutive year centroids
4. Drift score = `1 - similarity`

**Example calculation:**
```
NREGA 2019 centroid: [0.12, 0.34, ..., 0.67]  (384 dims)
NREGA 2020 centroid: [0.08, 0.51, ..., 0.62]
Cosine similarity: 0.26
Drift score: 1 - 0.26 = 0.74 → CRITICAL
```

**Classification thresholds:**
We calibrated these by looking at known major policy changes:
- NREGA 2020 COVID expansion: drift 0.74
- RTI 2019 amendment: drift 0.68
- PM-KISAN 2019 launch: drift 0.92 (first year, no comparison)

Thresholds:
- CRITICAL > 0.70: Found these corresponded to Acts passed, major budget changes
- HIGH 0.45-0.70: Significant amendments, new provisions
- MEDIUM 0.25-0.45: Operational changes, focus shifts
- LOW 0.10-0.25: Minor tweaks

**Limitations:**
1. Semantic drift ≠ substantive change. Could be just phrasing changes in data sources.
2. Sensitive to data volume. If 2019 has 3 chunks and 2020 has 30 chunks, centroid calculation is skewed.
3. Requires ≥ 2 years of data. Can't detect drift for NEP (only 2020-2025).

**Why this matters:**
Drift detection helps users understand "when did X change?" without manually reading 20 years of documents.

---

### Memory System (`src/memory.py`)

**Purpose:** Weight recent and frequently-accessed data higher.

**Time decay formula:**
```python
decay_weight = exp(-0.1 * age_years)
```

| Age | Weight |
|-----|--------|
| 0 years (current) | 1.00 |
| 1 year | 0.90 |
| 5 years | 0.61 |
| 10 years | 0.37 |
| 20 years | 0.14 |

**Access reinforcement:**
```python
access_boost = min(0.02 * access_count, 1.3)
final_weight = decay_weight * access_boost
```

Example: 5-year-old data accessed 20 times → weight 0.61 × 1.3 = 0.79

**Why cap at 1.3?**
Prevents runaway boosting where one popular chunk dominates forever.

**Trade-off:** Historical queries suffer
If user asks "When was RTI passed?", they need 2005 data, which has very low decay weight (0.11). We handle this by checking for temporal keywords ("when", "originally", "first") and disabling decay for such queries.

---

### Eligibility Checker (`src/eligibility.py`)

**Purpose:** Rule-based matching of user profiles to government schemes.

**Rules structure (example for PM-KISAN):**
```python
"PM-KISAN": {
    "description": "₹6000 annual income support for farmers",
    "rules": {
        "occupation": ["farmer"],
        "land_ownership": True,
        "income_max": 200000  # ₹2L annual
    },
    "priority": "HIGH"
}
```

**Matching logic:**
```python
eligible_schemes = []
for scheme, config in SCHEMES.items():
    if all_rules_match(user_profile, config["rules"]):
        eligible_schemes.append(scheme)
```

All rules must pass. No partial eligibility (confuses users).

**Why rule-based, not ML?**
1. Government eligibility is explicitly documented—no need to learn from data
2. No training data available for scheme eligibility classification
3. Rules are auditable and legally defensible
4. Easy to update when policies change (just edit the rules dict)

**Coverage:**
We have rules for 10 schemes. Real deployment would need rules for 100+ central schemes plus state schemes.

---

### Multimodal Input

**Image processing:**
```python
from PIL import Image
import pytesseract

image = Image.open(uploaded_file)
text = pytesseract.image_to_string(image)
```

Works well on: Printed Aadhaar cards, income certificates, land records
Fails on: Handwritten documents, low-quality scans, non-English text

**Audio processing:**
```python
import speech_recognition as sr

recognizer = sr.Recognizer()
with sr.AudioFile(audio_file) as source:
    audio_data = recognizer.record(source)
    text = recognizer.recognize_google(audio_data)
```

Uses Google Speech API (requires internet). No offline option currently.

**Video processing:**
```python
import av  # PyAV

# Extract audio track
container = av.open(video_file)
audio_stream = next(s for s in container.streams if s.type == 'audio')

# Convert to WAV in memory
wav_buffer = io.BytesIO()
# ... (decode audio, write to buffer)

# Transcribe
text = recognizer.recognize_google(audio_from_wav_buffer)
```

This was tricky—had to use in-memory buffers to avoid writing temp files. PyAV streaming API is poorly documented.

**Failure modes:**
- OCR on handwritten text: ~10% accuracy (tesseract limitation)
- Speech recognition with heavy accents: often fails
- Videos without audio track: crash (added check in final version)

---

## API Design

### Endpoints

| Endpoint | Method | Purpose | Rate Limit |
|----------|--------|---------|------------|
| `/query` | POST | Semantic search + reasoning | 20/min |
| `/drift` | POST | Policy evolution analysis | 20/min |
| `/recommendations` | POST | Related policies | 20/min |
| `/eligibility/check` | POST | User profile matching | 20/min |
| `/translate` | POST | Text translation | 200/min |
| `/tts` | POST | Text-to-speech audio | 100/min |
| `/document/upload` | POST | OCR + ingest | 10/min |
| `/stats` | GET | System statistics | 200/min |

**Rate limiting with `slowapi`:**
```python
from slowapi import Limiter
limiter = Limiter(key_func=get_remote_address)

@app.post("/query")
@limiter.limit("20/minute")
async def query_endpoint(...):
    ...
```

**Input validation:**
```python
from pydantic import BaseModel, Field

class QueryRequest(BaseModel):
    query_text: str = Field(..., min_length=3, max_length=500)
    top_k: int = Field(default=5, ge=1, le=10)
```

Pydantic automatically rejects invalid requests (returns HTTP 422).

---

## Security

**Current state (PROTOTYPE):**
- ❌ No authentication
- ❌ No HTTPS (runs on HTTP)
- ✅ Rate limiting
- ✅ Input validation
- ✅ Query length limits

**For government deployment needs:**
1. **Authentication:** Aadhaar-based or mobile OTP
2. **Authorization:** Role-based access (citizen, official, auditor)
3. **Audit logging:** All queries logged with user ID, timestamp, IP
4. **HTTPS:** TLS certificates for encrypted transport
5. **Input sanitization:** Guard against injection attacks (SQL, command, etc.)

We didn't implement auth for hackathon because:
- No real user data (all test queries)
- Single-user deployment (localhost only)
- Would add complexity without demonstrating core functionality

---

## Scalability

**Current limits (single instance):**
- ~50 concurrent users (FastAPI can handle, but ChromaDB writes are single-threaded)
- ~10 queries/second sustained
- ~100k documents in vector store before performance degrades

**Bottlenecks identified:**
1. **ChromaDB writes:** Single SQLite file, no concurrent writes
2. **Embedding model:** Loaded in memory (~400MB), one instance per process
3. **No caching:** Every query hits vector DB even for identical queries

**Horizontal scaling strategy (for production):**

```
                     Load Balancer
                          |
        +----------------+----------------+
        |                |                |
    API Server 1    API Server 2    API Server 3
        |                |                |
        +----------------+----------------+
                          |
                   Vector DB Cluster
                   (Qdrant or Pinecone)
```

**Steps:**
1. Migrate ChromaDB → Qdrant cluster (stateful, horizontally scalable)
2. Stateless API servers (no in-memory state, can add/remove freely)
3. Dedicated embedding service (GPU-accelerated, shared across API servers)
4. Redis cache for common queries (80% hit rate expected)

**Cost estimate (AWS, 1000 concurrent users):**
- 5× EC2 t3.medium for API: $180/month
- Qdrant cluster (3 nodes): $300/month
- Redis: $50/month
- **Total: ~$530/month**

This is assuming 100k queries/day, which is modest for a national service.

---

## Failure Handling

We documented failures during development and added handling:

| Failure | Cause | Handling |
|---------|-------|----------|
| ChromaDB corrupted | Schema change during ingestion | `fix_chromadb.bat` reset script |
| Embedding model OOM | Large batch size | Reduce to 32 chunks/batch |
| Translation timeout | Google API slow | 5s timeout, fallback to original text |
| Speech recognition fails | No internet | Error message, suggest text input |
| OCR returns gibberish | Handwritten docs | Show warning, allow manual override |
| Port 8000 in use | Previous server not closed | Error message with `netstat` command |

**Error response format:**
```json
{
    "error": "ChromaDB query failed",
    "detail": "Collection 'policy_data' not found",
    "suggestion": "Run 'python cli.py ingest-all' to initialize database"
}
```

---

## Tech Choices and Alternatives

### Why ChromaDB over Qdrant?

**ChromaDB:**
- ✅ Embedded (no Docker, no server)
- ✅ 5-minute setup
- ✅ Perfect for hackathon
- ❌ Single-machine limit
- ❌ SQLite corruption risk

**Qdrant:**
- ✅ Production-ready, horizontally scalable
- ✅ Better performance on large datasets
- ❌ Requires Docker or separate server
- ❌ More complex deployment

For hackathon: ChromaDB was right choice (demo priority).
For production: Would migrate to Qdrant.

### Why sentence-transformers over OpenAI embeddings?

**sentence-transformers:**
- ✅ MIT license, free
- ✅ Runs offline
- ✅ No API keys needed
- ✅ 384-dim (smaller than OpenAI's 1536-dim)
- ❌ Slightly lower quality on general text

**OpenAI embeddings:**
- ✅ State-of-the-art quality
- ❌ Costs $0.0004 per 1K tokens
- ❌ Requires internet and API key
- ❌ Privacy concerns for government data

For government use case, offline + free + no data leakage wins.

### Why FastAPI over Flask?

**FastAPI:**
- ✅ Async support (handle concurrent requests better)
- ✅ Auto-generated docs (`/docs` endpoint)
- ✅ Pydantic validation built-in
- ✅ Modern Python type hints

**Flask:**
- ✅ Simpler, more widespread
- ❌ Synchronous by default
- ❌ Manual validation needed

FastAPI gave us better performance and dev experience.

### Why Streamlit over React?

**Streamlit:**
- ✅ Built UI in 2 days
- ✅ Pure Python, no JS needed
- ✅ Auto-reload on code changes
- ❌ Limited customization
- ❌ Not great for complex UIs

**React:**
- ✅ Full UI flexibility
- ✅ Better performance
- ❌ Would take 1-2 weeks to build equivalent UI
- ❌ Separate frontend/backend deployment

For hackathon demo, Streamlit was the right tradeoff.

---

## Architecture Tradeoffs

### Retrieval-only vs RAG with LLM

**We chose retrieval-only:**
- ✅ Reproducible: same query → same answer
- ✅ No hallucination risk
- ✅ Fast (180ms)
- ✅ Free (no API costs)
- ✅ Auditable (exact source for every fact)
- ❌ Answers less fluent
- ❌ Can't synthesize across multiple sources

**RAG with LLM would give:**
- ✅ More natural language answers
- ✅ Better multi-hop reasoning
- ❌ 1-3 second latency
- ❌ Hallucination risk
- ❌ API costs

For government accountability, retrieval-only was the right call. You can trace every answer back to a specific document.

### Embedded vs client-server vector DB

**ChromaDB embedded:**
- ✅ Zero infrastructure
- ✅ Runs on laptop
- ❌ Single-machine limit
- ❌ No horizontal scaling

**Trade-off:** We accepted scalability limits for deployment simplicity. For a hackathon demo, this is fine. For production, would need distributed DB.

### Rule-based vs ML eligibility

**Rules:**
- ✅ Auditable
- ✅ Match official criteria exactly
- ✅ Easy to update
- ❌ Manual work to add new schemes

**ML classifier:**
- ✅ Could learn from examples
- ❌ No training data available
- ❌ Black box (can't explain decisions)
- ❌ Legally questionable ("why was I denied? because the model said so" doesn't work)

Rules are the right approach for government eligibility determination.

---

## Data Pipeline

**Ingestion (one-time setup):**
1. Raw data: CSVs and text files in `Data/`
2. `python cli.py ingest-all` reads files
3. For each policy:
   - Read budget CSV → chunks of 200 chars → embed → store in ChromaDB
   - Read news CSV → embed headlines + summaries → store
   - Read temporal text → split by year → embed → store
4. Add metadata: policy_id, year, modality, sentiment
5. Calculate initial decay weights

**Total ingestion time:** ~2 minutes for 847 chunks on Intel i5.

**Runtime updates:**
- On query: increment `access_count` for retrieved chunks
- On decay call: recalculate weights for all chunks (expensive, ~10 seconds)

---

## Where Things Can Break

**Known failure modes from testing:**

1. **ChromaDB corruption:** Happened twice. Caused by modifying schema during active ingestion. Fixed by adding proper initialization sequence.

2. **Memory overflow:** Embedding model loaded twice by accident (FastAPI + Streamlit both loading). Used 1.6GB RAM. Fixed by lazy loading.

3. **Translation timeout:** Google Translate API is slow in India (~2 second latency sometimes). Added 5s timeout and fallback to original text.

4. **OCR gibberish:** Handwritten documents return garbage. No fix, just warn user.

5. **Speech recognition offline:** Google Speech API needs internet. Fails silently if no connection. Added explicit error message.

6. **Port conflicts:** If previous server didn't shut down cleanly, port 8000 is busy. Added error message with troubleshooting steps.

---

## System Monitoring (What We'd Add for Production)

Currently we have:
- Console logs (not structured)
- `/stats` endpoint (basic counts)
- No alerting
- No performance tracking

Production would need:
- Structured logging (JSON format)
- Metrics: query latency p50/p95/p99, error rates, cache hit rates
- Alerts: High error rate, DB corruption, OOM
- Distributed tracing (if multi-service)
- User analytics: Most common queries, language distribution, policy breakdown

---

This is a prototype architecture. It demonstrates that semantic policy search is feasible with open-source tools and modest hardware. Production deployment would require hardening the data pipeline, authentication, distributed vector DB, and monitoring.
