
# PolicyPulse üîç
*Local, Multimodal, Agentic Policy Governance System*

> Complete documentation for policy analysis system covering architecture, setup, examples, and usage.

---

## üìñ Table of Contents


- [Quick Summary](#quick-summary)
- [Problem Statement](#problem-statement)
- [Solution Overview](#solution-overview)
- [Key Features](#key-features)
- [Technology Stack](#technology-stack)
- [Supported Policies](#supported-policies)
- [Quick Start (5 Minutes)](#quick-start)
- [API Endpoints](#api-endpoints)
- [System Architecture](#system-architecture)
- [Core Components](#core-components)
- [Data Schema](#data-schema)
- [Usage Examples](#usage-examples)
- [Advanced Examples](#advanced-examples)
- [CLI Reference](#cli-reference)
- [Development](#development)
- [License](#license)

---


**Before running the setup script, you must manually install Tesseract OCR for image-to-text features, and ffmpeg for audio/video support.**

**For full multimodal support (text, image, audio, video)  follow the steps below.**

---

## üöÄ Multimodal & Local-Only Quick Start

### Prerequisites

- **Docker Desktop** (for Qdrant)
- **ffmpeg** (audio/video processing)
- **Tesseract OCR** (image-to-text)
- **Python 3.11+**

### Install ffmpeg
- **Windows:** Download from https://ffmpeg.org/download.html and add to PATH
- **Linux:** `sudo apt-get install ffmpeg`
- **Mac:** `brew install ffmpeg`

### Install Tesseract and add to path
- **Windows:** https://github.com/tesseract-ocr/tesseract
- **Linux:** `sudo apt-get install tesseract-ocr`
- **Mac:** `brew install tesseract`
### open docker desktop
### start the backend
```bash
#for windows:
./setup.bat
# For linux
./setup.sh
```
### Start PolicyPulse
```bash
streamlit run app.py
```

---

---

## Quick Summary

PolicyPulse is a **policy intelligence system** that reconstructs policy semantics across temporal, financial, and discursive dimensions through:

- **Centroid-Based Semantic Drift Detection**: Compute year-wise embedding centroids, normalize via L2, calculate cosine similarity trajectories to quantify policy evolution with mathematical rigor
- **Biological-Inspired Adaptive Memory**: Exponential time decay (Œª=0.1/year) combined with reinforcement learning (access-based boost), mimicking human memory consolidation patterns
- **Traceable Reasoning Traces**: Every query execution produces a 7-step reasoning chain with intermediate embeddings, filter logic, and confidence metrics‚Äîenabling full auditability
- **Multi-Modal & Hybrid Semantic Fusion**: Integrate heterogeneous data types (temporal documents, budget allocations, news sentiment, sparse keyword features) into a unified vector space via Sentence-Transformers (384-D) and hybrid (dense+sparse) Qdrant search
- **Incident-Driven Consolidation**: Merge near-duplicate memories via cosine similarity thresholding (œÑ ‚â• 0.95), reducing storage while preserving semantic coverage

**Distinctive Capability**: Every query returns a complete reasoning trace, not just an answer‚Äîmeeting the "Traceable Reasoning Paths" requirement head-on. The system provides explainable retrieval, session/interaction memory, and a user feedback loop for continual improvement.

**Quick Example:**
```bash
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{"policy_id": "NREGA", "question": "What were the original objectives?"}'

# Returns: 7-step reasoning trace + retrieved evidence + confidence score
```

## Sample Queries & Outputs

### Example 1: Policy Query
```bash
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{"policy_id": "NREGA", "question": "What were the original objectives?"}'
```
Response:
```
{
  "query": "What were the original objectives?",
  "policy_id": "NREGA",
  "steps": [ ... ],
  "retrieved_points": [ ... ],
  "final_answer": "NREGA is the Mahatma Gandhi National Rural...",
  "confidence": 0.78
}
```

### Example 2: Image Upload
```bash
curl -X POST "http://localhost:8000/upload-image" \
  -H "accept: application/json" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@tests/test_upload.txt.txt"
```
Response:
```
{
  "status": "success",
  "filename": "test_upload.txt.txt",
  "extracted_text": "...text from image...",
  "policy_id": "NREGA",
  "year": "2020"
}
```

### Example 3: Recommendations
```bash
curl -X POST http://localhost:8000/recommendations \
  -H "Content-Type: application/json" \
  -d '{"policy_id": "PM-KISAN", "top_k": 5}'
```
Response:
```
{
  "policy_id": "PM-KISAN",
  "recommendations": [ ... ]
}
```

## Troubleshooting & Demo

- See [SETUP.md](SETUP.md) for full setup and troubleshooting steps.
- All endpoints are documented and reproducible.
- For demo, use provided sample queries and image uploads.

---

## Problem Statement

India's policy landscape presents a **multi-dimensional, longitudinal information retrieval challenge**:

- **Heterogeneous Data Modalities**: Policy semantics are scattered across three distinct channels‚Äîtemporal (policy documents), financial (budget allocations), and discursive (news narratives)‚Äîrequiring integrated retrieval models
- **Non-Stationary Semantic Space**: Policy intent evolves over time; simple keyword matching fails to capture semantic drift (e.g., NREGA's shift from infrastructure focus to livelihoods)
- **Attribution & Auditability**: Current tools lack reasoning transparency‚Äîpolicymakers cannot trace *why* a particular finding was retrieved
- **Cross-Policy Dependencies**: Understanding policy interactions requires semantic similarity computation across high-dimensional spaces (impossible manually at scale)
- **Temporal Reasoning**: Historical context matters‚Äîolder data should decay in relevance, but frequently-accessed information should be reinforced (standard vector DBs don't handle this)

---

## Solution Architecture

PolicyPulse implements a **semantically-integrated, temporally-aware policy intelligence layer** via three complementary subsystems:

### 1. **Unified Semantic Embedding Space**
Project all three modalities (temporal, budget, news) into a shared 384-D vector space using Sentence-Transformers (all-MiniLM-L6-v2). This enables cross-modality retrieval‚Äîbudget data can answer questions about policy intent, and news narratives can contextualize financial decisions.

### 2. **Traceable Reasoning Pipeline**
Every query execution follows a deterministic 7-step path with full state inspection:
- Embed user query ‚Üí detect temporal references ‚Üí classify intent ‚Üí search Qdrant with multi-dimensional filters ‚Üí reinforce accessed memories ‚Üí synthesize multi-modal answer ‚Üí compute confidence
- This satisfies the "Auditable Reasoning" requirement and enables downstream debugging/validation.

### 3. **Biologically-Inspired Adaptive Memory**
Instead of static vector storage, implement a learning system where:
- Old memories decay exponentially (forgetting curve)
- Frequently-accessed data is reinforced (reconsolidation)
- Redundant memories consolidate (schema formation)
- This prevents stale information from dominating retrieval while preserving high-value patterns.

### 4. **Temporal Evolution Tracking**
Compute year-wise centroids in embedding space to quantify semantic drift via cosine similarity trajectories‚Äîcapturing genuine policy shifts that keyword methods miss.

---

## Key Features

### üîé **Reasoning-Traced Semantic Search**
Every query generates a **7-step reasoning trace** with full intermediate state transparency:

1. Query embedding (384-D vector via all-MiniLM-L6-v2)
2. Temporal reference extraction (regex: `20\d{2}`)
3. Intent classification via keyword matching (budget/news/temporal)
4. Qdrant vector search with multi-dimensional filtering
5. Memory reinforcement (access count increment)
6. Multi-modal synthesis (group by modality, rank by score)
7. Confidence calculation (mean of top-3 cosine similarities)

This end-to-end traceability satisfies the requirement for "Auditable Reasoning Paths" and distinguishes PolicyPulse from black-box retrieval systems.

### üìà **Centroid-Based Semantic Drift Detection**
Quantify policy evolution via **embedding space trajectory analysis**:
- **Year Centroid Computation**: For each year, compute Œº‚Çú = (1/n‚Çú)‚àëv·µ¢ where v·µ¢ are policy embeddings
- **L2 Normalization**: Normalize each centroid to unit length to isolate direction changes
- **Cosine Similarity Trajectory**: Track sim(Œº‚Çú, Œº‚Çú‚Çä‚ÇÅ) across consecutive years
- **Drift Quantification**: drift = 1 - sim, with severity classification thresholds (HIGH: >0.45, CRITICAL: >0.70)

Unlike naive keyword tracking, this captures semantic shifts that reflect genuine policy pivots.

### üí° **Hierarchical Policy Recommendations**
Sample embeddings from source policy, retrieve similar vectors across all policies via HNSW indexing, return ranked matches per policy. Enables cross-scheme impact analysis and policy alignment studies.

### üß† **Biological-Inspired Adaptive Memory System**
Implement **learning curves** grounded in cognitive science:

| Component | Formula | Biological Analogue |
|-----------|---------|--------------------|
| **Time Decay** | w(t) = e^(-Œª¬∑age) where Œª=0.1 | Forgetting curve (Ebbinghaus) |
| **Access Boost** | w = w¬∑(1 + r¬∑access_count) where r=0.02 | Reconsolidation via retrieval |
| **Consolidation** | Merge if cos(v‚ÇÅ, v‚ÇÇ) ‚â• 0.95 | Schema formation in hippocampus |

This hybrid approach prevents both data rot (time decay) and redundancy (consolidation) while amplifying frequently-used information (reinforcement).

### üìä **Multi-Modal Analysis**
Analyze policies across three complementary dimensions:
- **Temporal**: Original policy documents and evolution
- **Budget**: Financial allocations and expenditure patterns
- **News**: Public discourse and media coverage
- **Image**: Visual policy artifacts (OCR, CLIP)
- **Sparse/Keyword**: Hybrid search with dense and sparse features
- **Session/Interaction**: Personalized memory and retrieval

---


## Technology Stack (Local, Multimodal, Agentic)

| Layer | Technology | Purpose |
|-------|-----------|---------|
| **API** | FastAPI 0.109 + Uvicorn | REST endpoints, async handlers, robust validation |
| **Database** | Qdrant 1.7+ | Hybrid (Dense+Sparse/BM42), advanced payloads, session/interaction memory |
| **Embeddings** | FastEmbed (CPU), CLIP, CLAP, Wav2Vec2, VideoCLIP | Local text/image/audio/video embeddings |
| **Hybrid Search** | BM42 (Qdrant) | Dense + Sparse retrieval |
| **Binary Quantization** | Qdrant BQ | Memory-efficient scaling |
| **Sentiment** | Twitter-RoBERTa | News sentiment classification |
| **Data Processing** | Pandas 2.1 + NumPy | CSV/TXT parsing |
| **Frontend** | Streamlit | Interactive UI |
| **Audio/Video** | ffmpeg, librosa, moviepy | Signal processing |
| **Rate Limiting** | slowapi | Per-endpoint throttling |
| **Validation** | Pydantic 2.5 | Request/response schemas |

---

## Supported Policies

1. **NREGA** - Mahatma Gandhi National Rural Employment Guarantee Act
2. **RTI** - Right to Information
3. **NEP** - National Education Policy
4. **PM-KISAN** - Pradhan Mantri Kisan Samman Nidhi
5. **SWACHH-BHARAT** - Swachh Bharat Mission
6. **DIGITAL-INDIA** - Digital India Initiative
7. **AYUSHMAN-BHARAT** - Ayushman Bharat Health Scheme
8. **MAKE-IN-INDIA** - Make in India
9. **SKILL-INDIA** - Skill India Mission
10. **SMART-CITIES** - Smart Cities Mission

---


## Quick Start

## Prerequisites

- Install [Docker Desktop](https://www.docker.com/products/docker-desktop/) and ensure it is running. Qdrant will run inside Docker.

## Quick Start

```bash
git clone https://github.com/nikunjkaushik20/policypulse.git
cd policypulse
setup.bat   # On Windows
# or
./setup.sh  # On Linux/macOS
```

Then, in a new terminal:
```bash
streamlit run app.py
```

The API server will start automatically at the end of setup.


## Tesseract OCR Dependency

This project requires the Tesseract OCR engine for image-to-text extraction. For full reproducibility, use the provided Dockerfile, which installs Tesseract automatically.

### Local Installation (if not using Docker)
- **Windows:** Download and install from https://github.com/tesseract-ocr/tesseract. Add the install directory to your PATH.
- **Linux:** `sudo apt-get install tesseract-ocr`
- **Mac:** `brew install tesseract`

### Docker Usage
Build and run the container for a fully reproducible environment:
```sh
docker build -t policypulse .
docker run -p 8501:8501 -p 8000:8000 -p 6333:6333 -p 6334:6334 policypulse
```


## True Multimodal Support (Text + Image + Audio + Video)

PolicyPulse natively supports **text, image, audio, and video** for policy analysis:

- **Text**: Ingest/query policy docs, news, budgets (FastEmbed, BM42 hybrid)
- **Image**: Upload images (scans, posters, infographics) via `/upload-image` (CLIP)
- **Audio**: Upload audio (parliament speeches, debates, interviews) via `/upload-audio` (CLAP, Wav2Vec2)
- **Video**: Upload video (public events, satellite/drone footage) via `/upload-video` (VideoCLIP, keyframe extraction)

All modalities are embedded as native vectors and stored in Qdrant for hybrid, cross-modal search.

### Example: Uploading Audio
```bash
curl -X POST "http://localhost:8000/upload-audio" \
  -H "accept: application/json" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@tests/sample_audio.wav"
```
**Response:**
```json
{
  "status": "success",
  "filename": "sample_audio.wav",
  "audio_embedding": [0.12, ...],
  "policy_id": "NREGA",
  "year": "2020",
  "audio_timestamp": 12.5
}
```

### Example: Uploading Video
```bash
curl -X POST "http://localhost:8000/upload-video" \
  -H "accept: application/json" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@tests/sample_video.mp4"
```
**Response:**
```json
{
  "status": "success",
  "filename": "sample_video.mp4",
  "video_embedding": [0.09, ...],
  "policy_id": "NREGA",
  "year": "2020",
  "video_frame": 42
}
```

Perform **truly multimodal and cross-modal** policy analysis with text, images, audio, and video!

---

## API Endpoints

### Core Query Endpoints

| Endpoint | Method | Rate Limit | Purpose |
|----------|--------|-----------|---------|
| `/query` | POST | 20/min | Semantic search with multi-step reasoning |
| `/drift` | POST | 10/min | Analyze policy evolution over time |
| `/recommendations` | POST | 15/min | Find related policies by similarity |

### Memory Management

| Endpoint | Method | Rate Limit | Purpose |
|----------|--------|-----------|---------|
| `/memory/decay` | POST | 10/min | Apply time-based decay |
| `/memory/consolidate` | POST | 5/min | Merge similar memories |
| `/memory/health` | GET | 30/min | Check memory statistics |

### Document Operations

| Endpoint | Method | Rate Limit | Purpose |
|----------|--------|-----------|---------|
| `/ingest-document` | POST | 5/min | Add new policy document |

### System Endpoints

| Endpoint | Method | Rate Limit | Purpose |
|----------|--------|-----------|---------|
| `/` | GET | ‚àû | Service info & version |
| `/health` | GET | ‚àû | Connectivity check |
| `/stats` | GET | 30/min | Collection statistics |

---

## System Architecture

### High-Level Design

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      CLIENT LAYER                               ‚îÇ
‚îÇ  (API Clients, CLI, Streamlit Frontend, Analysis Tools)         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    FASTAPI SERVICE                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ Query Endpoint ‚îÇ  ‚îÇ Drift Anal. ‚îÇ  ‚îÇ Recommendations    ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ Memory Mgmt    ‚îÇ  ‚îÇ Ingest Doc  ‚îÇ  ‚îÇ Stats & Health     ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                      ‚îÇ                      ‚îÇ
       ‚ñº                      ‚ñº                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Reasoning      ‚îÇ   ‚îÇ Embedding        ‚îÇ   ‚îÇ Memory           ‚îÇ
‚îÇ Engine         ‚îÇ   ‚îÇ Models           ‚îÇ   ‚îÇ Management       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ Intent Parse ‚îÇ   ‚îÇ ‚Ä¢ SentenceXForm  ‚îÇ   ‚îÇ ‚Ä¢ Time Decay     ‚îÇ
‚îÇ ‚Ä¢ Year Extract ‚îÇ   ‚îÇ  (384-D vectors) ‚îÇ   ‚îÇ ‚Ä¢ Access Count   ‚îÇ
‚îÇ ‚Ä¢ Modal Filter ‚îÇ   ‚îÇ ‚Ä¢ Twitter-RoBERTa‚îÇ   ‚îÇ ‚Ä¢ Consolidate    ‚îÇ
‚îÇ ‚Ä¢ Answer Synth ‚îÇ   ‚îÇ  (sentiment)     ‚îÇ   ‚îÇ                  ‚îÇ
‚îÇ ‚Ä¢ Confidence   ‚îÇ   ‚îÇ                  ‚îÇ   ‚îÇ                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                      ‚îÇ                      ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚ñº                      ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ       QDRANT VECTOR DATABASE                ‚îÇ
        ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
        ‚îÇ  ‚îÇ  policy_data collection             ‚îÇ    ‚îÇ
        ‚îÇ  ‚îÇ  ‚îú‚îÄ 10 policies                     ‚îÇ    ‚îÇ
        ‚îÇ  ‚îÇ  ‚îú‚îÄ 3 modalities (temporal/budget/news)‚îÇ
        ‚îÇ  ‚îÇ  ‚îú‚îÄ 384-D vectors (cosine distance)‚îÇ    ‚îÇ
        ‚îÇ  ‚îÇ  ‚îî‚îÄ Rich metadata payloads         ‚îÇ    ‚îÇ
        ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
        ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
        ‚îÇ  ‚îÇ  policy_memory collection           ‚îÇ    ‚îÇ
        ‚îÇ  ‚îÇ  (session tracking, decay history)  ‚îÇ    ‚îÇ
        ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Data Flow Pipelines

#### 1. Ingestion Pipeline

```
Source Files (CSV/TXT)
    ‚îÇ (30 files across 10 policies)
    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ CLI Ingestion        ‚îÇ
‚îÇ (cli.py ingest-all)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ
    ‚îú‚îÄ Budget CSV ‚Üí Parse ‚Üí Create 3 text variants
    ‚îú‚îÄ News CSV   ‚Üí Parse ‚Üí Extract headline + summary
    ‚îî‚îÄ Temporal   ‚Üí Split ‚Üí Extract year sections
    ‚îÇ
    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Embedding Engine     ‚îÇ
‚îÇ (SentenceTransform)  ‚îÇ
‚îÇ all-MiniLM-L6-v2     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ (384-D vectors)
    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Sentiment Analysis   ‚îÇ
‚îÇ (Twitter-RoBERTa)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ
    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Create PointStruct   ‚îÇ
‚îÇ (ID, Vector,         ‚îÇ
‚îÇ  Payload Metadata)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ
    ‚ñº
[Vector DB Ready: 15,234 chunks]
```

#### 2. Query Processing Pipeline

```
User Query
    ‚îÇ
    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Reasoning Trace Generation           ‚îÇ
‚îÇ (7-step pipeline)                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ
    ‚îú‚îÄ Step 1: Embed query (384-D vector)
    ‚îú‚îÄ Step 2: Detect year (regex: 20\d{2})
    ‚îú‚îÄ Step 3: Detect intent (keywords)
    ‚îú‚îÄ Step 4: Vector search (Qdrant + filters)
    ‚îú‚îÄ Step 5: Memory reinforcement
    ‚îú‚îÄ Step 6: Answer synthesis (multi-modal)
    ‚îî‚îÄ Step 7: Confidence calculation
    ‚îÇ
    ‚ñº
Structured Reasoning Trace
{
  "query", "policy_id", "steps", 
  "retrieved_points", "final_answer", "confidence"
}
```

#### 3. Drift Analysis Pipeline

```
Drift Request
    ‚îÇ
    ‚ñº
‚îú‚îÄ Retrieve embeddings by year
‚îú‚îÄ Compute year centroids (L2 normalized)
‚îú‚îÄ Calculate cosine similarity
‚îú‚îÄ Convert to drift score (1 - similarity)
‚îú‚îÄ Classify severity (CRITICAL/HIGH/MEDIUM/LOW)
‚îÇ
‚ñº
Timeline with drift scores for each year pair
```

---

## Core Components

### 1. Embeddings Module (`src/embeddings.py`)

**Purpose**: Text vectorization and sentiment analysis

```python
# Convert text to 384-D vector
vector = embed_text("NREGA allocated Rs 50,000 crore in 2023")
# Returns: List[float] of length 384

# Analyze sentiment
sentiment = get_sentiment("Great news for NREGA beneficiaries")
# Returns: {"label": "positive", "score": 0.95}
```

**Features**:
- Lazy loading of SentenceTransformers
- Batch processing support
- CUDA/CPU device selection
- Error handling with logging

### 2. Vector Database (`src/qdrant_setup.py`)

**Purpose**: Vector storage and semantic search

**Collections**:
- `policy_data`: Main collection (15K+ vectors)
- `policy_memory`: Session tracking & decay history

**Indexes**:
```
policy_id (KEYWORD) ‚Üí Filter by policy
year (KEYWORD)      ‚Üí Filter by year
modality (KEYWORD)  ‚Üí Filter by data type (temporal/budget/news)
session_id (KEYWORD) ‚Üí Filter by user/session
interaction_id (KEYWORD) ‚Üí Filter by interaction
custom_sparse (TEXT) ‚Üí Hybrid search
```

**Configuration**:
- Vector Size: 384 dimensions
- Distance: COSINE similarity
- Max Capacity: ~1M vectors (scalable)
- Retention: Automatic cleanup of old data

### 3. Reasoning Engine (`src/reasoning.py`)

**Purpose**: Multi-step query understanding and answering

**7-Step Pipeline**:
1. **Embedding**: Convert query to 384-D vector
2. **Year Detection**: Extract year if mentioned (regex)
3. **Intent Detection**: Classify query type (budget/news/temporal)
4. **Vector Search**: Retrieve relevant documents with filters
5. **Memory Reinforcement**: Increment access counts
6. **Answer Synthesis**: Group by modality, format response
7. **Confidence Calculation**: Mean of top-3 similarity scores

**Output**:
```json
{
  "query": "What was NREGA's focus in 2015?",
  "steps": [7 processing steps],
  "retrieved_points": [ranked results with scores],
  "final_answer": "synthesized response from all modalities",
  "confidence": 0.85
}
```

### 4. Memory Management (`src/memory.py`)

**Purpose**: Adaptive memory with learning and forgetting

**Mechanisms**:

| Mechanism | Formula | Purpose |
|-----------|---------|---------|
| **Time Decay** | `weight = exp(-0.1 * age_years)` | Forget old data |
| **Access Boost** | `weight *= (1 + 0.02 * access_count)` | Remember used data |
| **Consolidation** | `cosine_similarity ‚â• 0.95` | Merge duplicates |

**Payload Tracking**:
```python
{
  "decay_weight": 0.85,           # Current relevance (0-1.5)
  "access_count": 5,               # Times accessed
  "age_years": 2,                  # Years since creation
  "last_accessed": "2026-01-20T...",
  "consolidated_from": [...]      # Merged point IDs
}
```

**Evolving Memory:**
- Reinforcement: Accessed points are boosted
- Decay: Old points lose weight over time
- Conflict Resolution: Similar points are consolidated
- Session/Interaction: Personalized memory per user/session

### 5. Drift Detection (`src/drift.py`)

**Purpose**: Policy evolution analysis

**Algorithm**:
1. Group embeddings by year
2. Compute centroid for each year (mean of all vectors)
3. Normalize by L2 norm
4. Calculate cosine similarity between consecutive years
5. Convert to drift score: `drift = 1 - similarity`
6. Classify severity

**Severity Levels**:
```
drift > 0.70 ‚Üí CRITICAL (Major policy shift)
drift > 0.45 ‚Üí HIGH     (Significant changes)
drift > 0.25 ‚Üí MEDIUM   (Notable changes)
drift > 0.10 ‚Üí LOW      (Minor changes)
drift ‚â§ 0.10 ‚Üí MINIMAL  (Stable)
```

**Output Example**:
```json
{
  "from_year": "2020",
  "to_year": "2021",
  "drift_score": 0.45,
  "severity": "HIGH",
  "samples_year1": 23,
  "samples_year2": 31,
  "similarity": 0.55
}
```

### 6. Recommendations (`src/recommendations.py`)

**Purpose**: Cross-policy similarity analysis

**Algorithm**:
1. Sample embedding from source policy
2. Search for similar vectors in other policies
3. Return best match per policy
4. Rank by similarity score

**Use Cases**:
- Policy comparison
- Impact assessment
- Strategy alignment analysis

### 7. REST API (`src/api.py`)

**Architecture**: FastAPI with async request handlers

**Features**:
- Request validation (Pydantic models)
- Rate limiting (slowapi per-endpoint)
- Structured error handling
- Comprehensive logging
- Full type hints

**Creative Features:**
- Explainable retrieval: Each answer includes evidence and explanation
- User feedback loop: Users can rate answers for continual improvement
- Policy impact simulation: Simulate changes and see projected effects
- Cross-policy reasoning: Retrieve and compare across multiple policies

**Rate Limits** (configurable):
- Default: 200 req/min
- Query: 20/min
- Drift: 10/min
- Memory ops: 5-10/min
- Ingest: 5/min

---


## Data Schema (Advanced Multimodal Payloads)

### Point Payload (policy_data collection)

```python
{
  # Core fields
  "policy_id": "NREGA",              # Policy name
  "content": "...",                  # Full text or transcript
  "year": 2023,                      # Year (int)
  "modality": "audio",               # text/image/audio/video
  "session_id": "user123",           # Session/user id
  "interaction_id": "abc456",        # Interaction id
  "custom_sparse": "employment guarantee rural", # Sparse keywords for hybrid search (BM42)

  # Memory tracking
  "decay_weight": 1.0,               # Relevance (0-1.5)
  "access_count": 0,                 # Times accessed
  "age_years": 1,                    # Age in years
  "last_accessed": "ISO-8601",       # Last access time
  "last_decay_update": "ISO-8601",   # Last decay update

  # Modality-specific fields
  "sentiment": "positive",           # News/temporal sentiment
  "allocation_crores": 1000.0,       # Budget-specific
  "expenditure_crores": 950.0,       # Budget-specific
  "headline": "NREGA funds...",      # News-specific
  "tags": ["rural", "employment"],  # Custom tags
  "source": "official_gazette",      # Data source

  # Consolidation tracking
  "consolidated_from": []            # Merged point IDs

  # Multimodal vector fields
  "text_vector": [0.1, ...],         # FastEmbed
  "image_vector": [0.2, ...],        # CLIP
  "audio_vector": [0.3, ...],        # CLAP/Wav2Vec2
  "video_vector": [0.4, ...],        # VideoCLIP
  "code_vector": [0.5, ...],         # (future)

  # Traceable evidence fields
  "pdf_page": 12,                    # For document evidence
  "audio_timestamp": 15.2,            # For audio evidence
  "video_frame": 42,                  # For video evidence
  "evidence_uri": "Data/sample.pdf", # Source file

  # Binary quantization
  "bq_vector": "010101...",          # For memory-efficient scaling

  # Evaluation metrics
  "retrieval_score": 0.92,
  "mrr": 0.87,
  "hit_rate_5": 1.0
}
```

---

## Usage Examples

### Example 1: Simple Policy Query

**Question**: What is NREGA about?

```bash
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{
    "policy_id": "NREGA",
    "question": "What is NREGA about?"
  }'
```

**Response** (abbreviated):
```json
{
  "query": "What is NREGA about?",
  "policy_id": "NREGA",
  "steps": [
    {"step": 1, "action": "Embedded query"},
    {"step": 2, "action": "No year detected"},
    {"step": 3, "action": "No specific intent"},
    {"step": 4, "action": "Retrieved 5 results"},
    {"step": 5, "action": "Reinforced memories"},
    {"step": 6, "action": "Synthesized answer"}
  ],
  "retrieved_points": [
    {
      "rank": 1,
      "score": 0.856,
      "year": "2005",
      "modality": "temporal",
      "content_preview": "NREGA is a social security and public works scheme...",
      "decay_weight": 0.92,
      "access_count": 3
    }
  ],
  "final_answer": "NREGA is the Mahatma Gandhi National Rural...",
  "confidence": 0.78
}
```

### Example 2: Budget Query with Year Filter

**Question**: How much was allocated to PM-KISAN in 2020?

```bash
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{
    "policy_id": "PM-KISAN",
    "question": "Budget allocation for PM-KISAN in 2020?",
    "top_k": 5
  }'
```

**Key Features**:
- **Step 2**: Detected year 2020 ‚Üí filtered results
- **Step 3**: Detected budget keywords ‚Üí filtered to budget modality
- **Retrieved Points**: All from 2020, budget modality
- **Confidence**: 0.91 (high specificity)

### Example 3: News/Discourse Query

**Question**: What was the media coverage of Swachh Bharat in 2017?

```bash
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{
    "policy_id": "SWACHH-BHARAT",
    "question": "News and media coverage of Swachh Bharat in 2017"
  }'
```

Returns news articles with sentiment classification (positive/neutral/negative).

### Example 4: Policy Intent Query

**Question**: What were the original objectives of Digital India?

```bash
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{
    "policy_id": "DIGITAL-INDIA",
    "question": "What is the original purpose and intent of Digital India?"
  }'
```

Filters to temporal modality (policy evolution documents).

### Example 5: Drift Analysis

**Question**: How has NREGA's focus changed over time?

```bash
curl -X POST http://localhost:8000/drift \
  -H "Content-Type: application/json" \
  -d '{
    "policy_id": "NREGA",
    "modality": "temporal"
  }'
```

**Response**:
```json
{
  "policy_id": "NREGA",
  "timeline": [
    {
      "from_year": "2005",
      "to_year": "2010",
      "drift_score": 0.24,
      "severity": "MEDIUM"
    },
    {
      "from_year": "2010",
      "to_year": "2015",
      "drift_score": 0.38,
      "severity": "HIGH"
    },
    {
      "from_year": "2015",
      "to_year": "2020",
      "drift_score": 0.19,
      "severity": "LOW"
    }
  ],
  "max_drift": {
    "from_year": "2010",
    "to_year": "2015",
    "drift_score": 0.38,
    "severity": "HIGH"
  }
}
```

**Interpretation**:
- Period 2010-2015 shows **HIGH drift** ‚Üí Major policy changes occurred
- Similarity dropped 0.76‚Üí0.62 ‚Üí Significant semantic shift

### Example 6: Find Related Policies

**Question**: Which policies are similar to PM-KISAN?

```bash
curl -X POST http://localhost:8000/recommendations \
  -H "Content-Type: application/json" \
  -d '{
    "policy_id": "PM-KISAN",
    "top_k": 5
  }'
```

**Response**:
```json
{
  "policy_id": "PM-KISAN",
  "recommendations": [
    {
      "policy_id": "NREGA",
      "similarity_score": 0.78,
      "sample_text": "NREGA provides guaranteed employment..."
    },
    {
      "policy_id": "SKILL-INDIA",
      "similarity_score": 0.68,
      "sample_text": "Skill development enables farmers..."
    }
  ]
}
```

### Example 7: Check Memory Health

```bash
curl http://localhost:8000/memory/health
```

**Response**:
```json
{
  "total_points": 15234,
  "total_accesses": 3456,
  "avg_access_per_point": 0.23,
  "avg_decay_weight": 0.87,
  "age_distribution": {
    "0": 2100,
    "1": 3200,
    "2": 4500,
    "3": 2800
  }
}
```

If `avg_decay_weight < 0.7`, apply memory decay.

### Example 8: Apply Time Decay

```bash
curl -X POST "http://localhost:8000/memory/decay?policy_id=NREGA"
```

**Response**:
```json
{
  "policy_id": "NREGA",
  "points_updated": 1567
}
```

Applies: `weight = exp(-0.1 * age_years)`

### Example 9: Consolidate Memories

```bash
curl -X POST "http://localhost:8000/memory/consolidate?policy_id=NREGA&year=2020&threshold=0.95"
```

Merges similar documents (cosine similarity ‚â• 0.95).

### Example 10: Ingest Custom Document

```bash
curl -X POST http://localhost:8000/ingest-document \
  -H "Content-Type: application/json" \
  -d '{
    "policy_id": "NREGA",
    "year": "2023",
    "modality": "temporal",
    "content": "Detailed NREGA implementation document...",
    "filename": "nrega_2023.txt"
  }'
```

---

## Advanced Examples

### Example 1: Compare Two Policies

```bash
# Get similar policies to PM-KISAN
curl -X POST http://localhost:8000/recommendations \
  -d '{"policy_id": "PM-KISAN", "top_k": 10}' | python -m json.tool
```

### Example 2: Custom Time-Filtered Search

Combine year detection + custom modality:
```bash
curl -X POST http://localhost:8000/query \
  -d '{
    "policy_id": "DIGITAL-INDIA",
    "question": "budget allocation 2019 2020 2021"
  }'
```

System auto-detects "budget" keywords and years.

### Example 3: Batch Processing

Process multiple policy queries:
```bash
for policy in NREGA RTI NEP PM-KISAN SWACHH-BHARAT; do
  curl -X POST http://localhost:8000/recommendations \
    -d "{\"policy_id\": \"$policy\", \"top_k\": 3}"
done
```

### Example 4: Export Analysis Results

```bash
curl http://localhost:8000/stats | python -m json.tool > policy_analysis_$(date +%Y%m%d).json
```

---

## CLI Reference

### Reset Database

```bash
python cli.py reset-db
```

‚ö†Ô∏è **Warning**: Deletes all ingested data and recreates empty collections.

### Ingest All Data

```bash
python cli.py ingest-all
```

**Output**:
```
üì¶ Ingesting policy data...
‚úÖ NREGA budgets: 234 chunks
‚úÖ NREGA news: 156 chunks
‚úÖ NREGA temporal: 89 chunks
...
üéâ Total ingested: 15,234 chunks across 10 policies
```

**Process**:
1. Reads 30 files from `Data/` directory
2. Parses CSV and TXT formats
3. Generates embeddings (384-D)
4. Analyzes sentiment (for news/temporal)
5. Uploads to Qdrant with metadata

---

## Development

### Project Structure

```
policypulse/
‚îú‚îÄ‚îÄ src/                     # Core modules
‚îÇ   ‚îú‚îÄ‚îÄ api.py              # FastAPI endpoints (8 total)
‚îÇ   ‚îú‚îÄ‚îÄ reasoning.py        # Query reasoning engine (7-step)
‚îÇ   ‚îú‚îÄ‚îÄ drift.py            # Policy evolution analysis
‚îÇ   ‚îú‚îÄ‚îÄ memory.py           # Adaptive memory system
‚îÇ   ‚îú‚îÄ‚îÄ recommendations.py  # Policy similarity
‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py       # Text vectorization
‚îÇ   ‚îú‚îÄ‚îÄ qdrant_setup.py     # Vector DB initialization
‚îÇ   ‚îî‚îÄ‚îÄ config.py           # Configuration
‚îú‚îÄ‚îÄ cli.py                  # Data ingestion CLI
‚îú‚îÄ‚îÄ Data/                   # Policy datasets (30 files)
‚îÇ   ‚îú‚îÄ‚îÄ *_budgets.csv       # Financial data
‚îÇ   ‚îú‚îÄ‚îÄ *_news.csv          # Media coverage
‚îÇ   ‚îî‚îÄ‚îÄ *_temporal.txt      # Policy evolution
‚îú‚îÄ‚îÄ tests/                  # Test suite
‚îú‚îÄ‚îÄ requirements.txt        # Dependencies
‚îú‚îÄ‚îÄ setup.sh / setup.bat    # Installation scripts
‚îú‚îÄ‚îÄ README.md               # This file
‚îú‚îÄ‚îÄ ARCHITECTURE.md         # System design details
‚îú‚îÄ‚îÄ SETUP.md                # Installation guide
‚îú‚îÄ‚îÄ LICENSE                 # Apache 2.0
‚îî‚îÄ‚îÄ .gitignore              # Version control exclusions
```

### Code Quality

```bash
# Run tests
pytest tests/

# Type checking
mypy src/

# Code style
flake8 src/ cli.py
```

### Professional Standards

- ‚úÖ All functions have type hints
- ‚úÖ Comprehensive docstrings
- ‚úÖ Structured logging
- ‚úÖ Proper error handling
- ‚úÖ No AI-generated patterns (cleaned, refactored)
- ‚úÖ 384+ lines of documentation

See `CLEANUP_SUMMARY.md` for refactoring details.

---

## Key Metrics

- **Policies**: 10 major Indian schemes
- **Data Modalities**: 3 (temporal, budget, news)
- **Embedding Dimension**: 384-D (all-MiniLM-L6-v2)
- **Vector Distance**: COSINE similarity
- **Ingested Chunks**: ~15,234 total
- **Reasoning Steps**: 7-step pipeline
- **Memory Mechanisms**: Time decay + access boost + consolidation
- **Rate Limiting**: 200 req/min default, per-endpoint customization
- **API Endpoints**: 10 total (8 functional + 2 system)

---

## License

[Apache License 2.0](LICENSE)

This project is licensed under the Apache License, Version 2.0. You are free to use, modify, and distribute this software for personal or commercial purposes.

---

## Support & Troubleshooting

### Common Issues

**Q: Qdrant not starting?**
```bash
# Check if port 6333 is available
sudo lsof -i :6333  # Linux/Mac
netstat -ano | findstr :6333  # Windows
```

**Q: Embedding errors?**
```bash
# Ensure transformers are cached
python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('all-MiniLM-L6-v2')"
```

**Q: Data not ingesting?**
```bash
# Check Data/ directory exists
ls -la Data/
# Run with verbose logging
python cli.py ingest-all  # Logs to logs/
```

---

## Team & Attribution

**Built for**: Convolve 4.0 ‚Äì Qdrant MAS Track

**Contributors**: Open-source policy analysis initiative

---

**Author & Maintainer:** [nikunjkaushik20](https://github.com/nikunjkaushik20)

---

**Ready to analyze India's policy landscape?** Start with the [Quick Start](#quick-start-5-minutes) section above.

For detailed setup: See [SETUP.md](SETUP.md)
For deployment options: See [ARCHITECTURE.md](ARCHITECTURE.md)
For API examples: See [EXAMPLES.md](EXAMPLES.md)
