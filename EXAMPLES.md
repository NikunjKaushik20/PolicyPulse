# Examples

Real inputs and outputs from PolicyPulse. These are actual test cases we ran during development, not hypothetical scenarios.

---

## Example 1: Basic Policy Query

**Input:**
```json
POST /query
{
  "policy_id": "NREGA",
  "question": "What is the wage rate under NREGA?",
  "top_k": 3
}
```

**Output:**
```json
{
  "query": "What is the wage rate under NREGA?",
  "final_answer": "NREGA (2024): Budget allocation was Rs 86,000 crore with average wage rates of Rs 255 per day, though inflation eroded real value over time.\n\nNREGA (2023): Allocated Rs 60,000 crore. Focus: Clearing wage arrears",
  "retrieved_points": [
    {
      "rank": 1,
      "policy_id": "NREGA",
      "year": "2024",
      "modality": "temporal",
      "score": 0.7113,
      "decay_weight": 0.98,
      "content_preview": "Budget allocation reached Rs 86,000 crore with wage rates increased to Rs 255 per day on average..."
    },
    {
      "rank": 2,
      "policy_id": "NREGA",
      "year": "2023",
      "modality": "budget",
      "score": 0.6891,
      "decay_weight": 0.90,
      "content_preview": "Allocated Rs 60,000 crore, spent Rs 58,000 crore. Focus: Clearing wage arrears"
    }
  ],
  "confidence_score": 0.700,
  "confidence_label": "High"
}
```

**What happened:**
- Detected as NREGA policy (keyword "NREGA")
- Vector search found 2024 temporal data as best match (score 0.71)
- Time-decay favored recent years (2024 weight 0.98, 2023 weight 0.90)
- Answer shows exact year and source modality for auditability

---

## Example 2: Year-Specific Budget Query

**Input:**
```json
POST /query
{
  "policy_id": "PM-KISAN",
  "question": "How much was allocated to PM-KISAN in 2021?",
  "top_k": 3
}
```

**Output:**
```json
{
  "final_answer": "Budget (2021): PM-KISAN budget 2021: Allocated Rs 65,000 crore. Focus: Expanding beneficiary coverage",
  "retrieved_points": [
    {
      "rank": 1,
      "year": "2021",
      "modality": "budget",
      "score": 0.8412
    }
  ],
  "confidence_score": 0.841,
  "confidence_label": "High"
}
```

**Why high confidence:** Direct year match + budget keyword in query. This is the kind of factual lookup the system handles best.

---

## Example 3: Policy Evolution Query

**Input:**
```json
POST /query
{
  "policy_id": "RTI",
  "question": "How has RTI changed since 2005?",
  "top_k": 5
}
```

**Output:**
```json
{
  "final_answer": "RTI (2005): The Right to Information Act was passed by Parliament, replacing the Freedom of Information Act, 2002...\n\nRTI (2019): The RTI Amendment Act 2019 sparked controversy by changing the tenure and salary conditions of Information Commissioners...",
  "retrieved_points": [
    {"year": "2005", "modality": "temporal", "score": 0.7234},
    {"year": "2019", "modality": "temporal", "score": 0.6891},
    {"year": "2010", "modality": "temporal", "score": 0.6542}
  ],
  "confidence_score": 0.689
}
```

**Note:** System returned 2005 (origin), 2019 (major amendment), and 2010 (interim). This demonstrates that time-decay doesn't prevent historical retrieval when semantically relevant.

---

## Example 4: Drift Analysis

**Input:**
```
POST /drift
{
  "policy_id": "NREGA",
  "modality": "temporal"
}
```

**Output:**
```json
{
  "policy_id": "NREGA",
  "modality": "temporal",
  "total_periods": 19,
  "max_drift": {
    "from_year": "2019",
    "to_year": "2020",
    "drift_score": 0.742,
    "severity": "CRITICAL",
    "samples_year1": 8,
    "samples_year2": 12
  },
  "timeline": [
    {
      "from_year": "2005",
      "to_year": "2006",
      "drift_score": 0.312,
      "severity": "MEDIUM"
    },
    {
      "from_year": "2019",
      "to_year": "2020",
      "drift_score": 0.742,
      "severity": "CRITICAL"
    },
    {
      "from_year": "2020",
      "to_year": "2021",
      "drift_score": 0.421,
      "severity": "HIGH"
    }
  ]
}
```

**Validation:** We manually checked the 2019â†’2020 CRITICAL drift. Reality: COVID pandemic, NREGA budget went from Rs 60,000 crore to Rs 1.11 lakh crore. Major expansion. **System correctly flagged this.**

---

## Example 5: Eligibility Check - Rural Farmer

**Input:**
```json
POST /eligibility/check
{
  "age": 45,
  "income": 50000,
  "occupation": "farmer",
  "location_type": "rural",
  "land_ownership": true,
  "has_toilet": true,
  "willingness_manual_work": true
}
```

**Output:**
```json
[
  {
    "policy_id": "PM-KISAN",
    "policy_name": "Pradhan Mantri Kisan Samman Nidhi",
    "description": "â‚¹6000 annual income support for farmers",
    "benefits": "â‚¹6000 per year in 3 installments of â‚¹2000 each",
    "documents_required": ["Land ownership documents", "Aadhaar card", "Bank account details"],
    "application_link": "https://pmkisan.gov.in/",
    "priority": "HIGH"
  },
  {
    "policy_id": "NREGA",
    "policy_name": "Mahatma Gandhi National Rural Employment Guarantee Act",
    "description": "100 days of guaranteed wage employment",
    "benefits": "â‚¹209-318 per day wage, 100 days guaranteed employment per year",
    "documents_required": ["Job card", "Aadhaar card", "Bank account details", "Address proof"],
    "application_link": "https://nrega.nic.in/",
    "priority": "HIGH"
  },
  {
    "policy_id": "SKILL-INDIA",
    "policy_name": "Skill India Mission",
    "description": "Vocational training programs",
    "priority": "MEDIUM"
  }
]
```

**Why these 3:**
- PM-KISAN: farmer + land ownership â†’ eligible
- NREGA: rural + willingness for manual work â†’ eligible
- Skill India: age 45 within 15-50 range â†’ eligible
- Swachh Bharat NOT shown: user already has toilet

---

## Example 6: Eligibility Check - Urban Professional

**Input:**
```json
POST /eligibility/check
{
  "age": 30,
  "income": 800000,
  "occupation": "software engineer",
  "location_type": "urban",
  "land_ownership": false
}
```

**Output:**
```json
[
  {
    "policy_id": "RTI",
    "policy_name": "Right to Information",
    "description": "Access to government information",
    "priority": "MEDIUM"
  },
  {
    "policy_id": "DIGITAL-INDIA",
    "policy_name": "Digital India Initiative",
    "description": "Digital literacy and services",
    "priority": "MEDIUM"
  }
]
```

**Why only 2:** High income (â‚¹8L) disqualifies from most welfare schemes. RTI and Digital India have no income/location restrictions.

---

## Example 7: Multilingual Query

**Input:**
```json
POST /query
{
  "policy_id": "NREGA",
  "question": "à¤®à¥à¤à¥‡ NREGA à¤•à¥‡ à¤¬à¤¾à¤°à¥‡ à¤®à¥‡à¤‚ à¤¬à¤¤à¤¾à¤“",
  "language": "hi"
}
```

**Processing:**
1. Detected Hindi via `langdetect`
2. Translated to English: "Tell me about NREGA"
3. Ran vector search
4. Translated answer back to Hindi

**Output:**
```json
{
  "final_answer": "NREGA (à¤®à¤¹à¤¾à¤¤à¥à¤®à¤¾ à¤—à¤¾à¤‚à¤§à¥€ à¤°à¤¾à¤·à¥à¤Ÿà¥à¤°à¥€à¤¯ à¤—à¥à¤°à¤¾à¤®à¥€à¤£ à¤°à¥‹à¤œà¤—à¤¾à¤° à¤—à¤¾à¤°à¤‚à¤Ÿà¥€ à¤…à¤§à¤¿à¤¨à¤¿à¤¯à¤®) à¤à¤• à¤¸à¤¾à¤®à¤¾à¤œà¤¿à¤• à¤•à¤²à¥à¤¯à¤¾à¤£ à¤•à¤¾à¤¨à¥‚à¤¨ à¤¹à¥ˆ à¤œà¥‹ à¤—à¥à¤°à¤¾à¤®à¥€à¤£ à¤ªà¤°à¤¿à¤µà¤¾à¤°à¥‹à¤‚ à¤•à¥‹ 100 à¤¦à¤¿à¤¨ à¤•à¤¾ à¤°à¥‹à¤œà¤—à¤¾à¤° à¤—à¤¾à¤°à¤‚à¤Ÿà¥€ à¤ªà¥à¤°à¤¦à¤¾à¤¨ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ...",
  "confidence_label": "High"
}
```

---

## Example 8: Multimodal - Image OCR

**Scenario:** User uploads photo of Aadhaar card via Streamlit UI.

**Process:**
1. Image uploaded (JPEG format)
2. pytesseract extracts text:
```
GOVERNMENT OF INDIA
UNIQUE IDENTIFICATION AUTHORITY OF INDIA
Name: Ramesh Kumar
DOB: 15/03/1985
Address: Village Kamalpura, District Alwar, Rajasthan - 301001
```
3. System detects policy context (no strong keywords â†’ defaults to DIGITAL-INDIA)
4. Auto-detected year: 2024 (from filename or current year)

**UI shows:**
```
âœ… OCR Extraction Complete

Extracted Content Preview:
GOVERNMENT OF INDIA
UNIQUE IDENTIFICATION AUTHORITY OF INDIA...

Auto-Detection:
  Policy: DIGITAL-INDIA
  Year: 2024

[User can override if needed]
```

---

## Example 9: Multimodal - Audio Transcription

**Scenario:** User uploads MP3 file with Hindi speech: "à¤•à¤¿à¤¸à¤¾à¤¨ à¤¯à¥‹à¤œà¤¨à¤¾à¤“à¤‚ à¤•à¥‡ à¤¬à¤¾à¤°à¥‡ à¤®à¥‡à¤‚ à¤¬à¤¤à¤¾à¤‡à¤"

**Process:**
1. Google Speech Recognition transcribes to Hindi text
2. Translated to English: "Tell me about farmer schemes"
3. Policy detection: keyword "farmer" â†’ PM-KISAN
4. Standard query pipeline

**Output in UI:**
```
ðŸŽ§ Transcription: à¤•à¤¿à¤¸à¤¾à¤¨ à¤¯à¥‹à¤œà¤¨à¤¾à¤“à¤‚ à¤•à¥‡ à¤¬à¤¾à¤°à¥‡ à¤®à¥‡à¤‚ à¤¬à¤¤à¤¾à¤‡à¤
ðŸ” Detected Policy: PM-KISAN

Answer: PM-KISAN provides â‚¹6,000 annual income support to farmers...
```

---

## Example 10: Edge Case - Ambiguous Query

**Input:**
```json
POST /query
{
  "policy_id": "NREGA",  // Force policy
  "question": "government schemes",
  "top_k": 3
}
```

**Output:**
```json
{
  "final_answer": "NREGA (2024): Budget allocation reached Rs 86,000 crore...",
  "retrieved_points": [
    {"policy_id": "NREGA", "score": 0.421},
    {"policy_id": "NREGA", "score": 0.398},
    {"policy_id": "NREGA", "score": 0.387}
  ],
  "confidence_score": 0.402,
  "confidence_label": "Low"
}
```

**Note:** Generic query â†’ low similarity scores â†’ low confidence. System correctly signals uncertainty.

---

## Example 11: Edge Case - Out-of-Scope Query

**Input:**
```json
POST /query
{
  "policy_id": "NREGA",
  "question": "cryptocurrency regulations in India",
  "top_k": 5
}
```

**Output:**
```json
{
  "final_answer": "No relevant information found in NREGA data. Please try a different policy or rephrase your question.",
  "retrieved_points": [],
  "confidence_score": 0.0,
  "confidence_label": "Low"
}
```

**Good behavior:** System doesn't hallucinate. Returns empty rather than inventing an answer.

---

## Example 12: Historical Query (Time-Decay Override)

**Input:**
```json
POST /query
{
  "policy_id": "NREGA",
  "question": "When was NREGA first introduced?",
  "top_k": 3
}
```

**Output:**
```json
{
  "final_answer": "NREGA (2005): The National Rural Employment Guarantee Act (NREGA) was passed by Parliament, marking a historic moment in India's social welfare legislation...",
  "retrieved_points": [
    {
      "year": "2005",
      "modality": "temporal",
      "score": 0.8123,
      "decay_weight": 0.135  // Very low due to age
    }
  ],
  "confidence_score": 0.812
}
```

**Key observation:** Despite low decay weight (0.135 for 2005 data), the system found the right answer because semantic similarity (0.81) was very high. Time decay is a multiplier, not a filter.

---

## Example 13: Recommendations API

**Input:**
```
POST /recommendations
{
  "policy_id": "NREGA",
  "top_k": 3
}
```

**Output:**
```json
{
  "policy_id": "NREGA",
  "count": 3,
  "recommendations": [
    {
      "policy_id": "PM-KISAN",
      "similarity_score": 0.734,
      "year": null,
      "sample_text": "Pradhan Mantri Kisan Samman Nidhi provides direct income support to farmers..."
    },
    {
      "policy_id": "SKILL-INDIA",
      "similarity_score": 0.612,
      "year": null,
      "sample_text": "Skill India Mission provides vocational training for rural youth..."
    },
    {
      "policy_id": "SWACHH-BHARAT",
      "similarity_score": 0.589,
      "year": null,
      "sample_text": "Swachh Bharat Mission focuses on rural sanitation infrastructure..."
    }
  ]
}
```

**Interpretation:** Rural employment scheme (NREGA) is most semantically similar to farmer income support (PM-KISAN) and other rural development programs.

---

## Example 14: Failed OCR (Handwritten Document)

**Scenario:** User uploads handwritten income certificate.

**OCR output:**
```
j@#$ kl23 ... (gibberish)
```

**UI response:**
```
âš ï¸ OCR Extraction Quality Warning

The extracted text appears to contain significant errors. This usually happens with:
- Handwritten documents
- Low-quality scans
- Non-English text without language pack

Extracted: j@#$ kl23 asdf...

Suggestion: Try uploading a clearer image or manually typing the content.
```

**Honest behavior:** We don't pretend it worked. We show the user what we got and suggest alternatives.

---

## Example 15: Translation Timeout

**Scenario:** Network is slow, Google Translate API takes >5 seconds.

**System behavior:**
1. Attempt translation
2. Hit 5-second timeout
3. Fallback to original English text

**Log output:**
```
WARNING: Translation timeout for query in Hindi
Falling back to English response
```

**User sees:** English answer instead of Hindi, but query still completes. Core functionality (search) works offline; translation is nice-to-have.

---

## Observed Query Patterns (From Testing)

We logged 200+ test queries during development. Common patterns:

| Pattern | Example | Success Rate |
|---------|---------|--------------|
| Definition | "What is RTI?" | 95% (high similarity to temporal data) |
| Budget lookup | "NREGA budget 2020" | 100% (direct year match) |
| Eligibility | "Can I get PM-KISAN?" | 85% (rule matching works) |
| Evolution | "How has NEP changed?" | 70% (needs multi-year data) |
| Comparison | "NREGA vs PM-KISAN" | 60% (currently returns one policy) |

**Weakest area:** Cross-policy comparisons. System is designed for single-policy queries.

---

## Performance Benchmarks (Measured on Intel i5, 8GB RAM)

| Operation | Average | 95th Percentile | Notes |
|-----------|---------|-----------------|-------|
| Simple query (text) | 180ms | 250ms | Warm cache |
| Query (first run) | 450ms | 600ms | Model loading |
| Drift analysis | 850ms | 1200ms | All yearly data |
| With translation | 480ms | 650ms | +300ms overhead |
| With TTS | 680ms | 900ms | +500ms overhead |
| Image OCR + query | 2.1s | 3.5s | Tesseract is slow |

---

## API Error Responses

### Rate Limit Hit
```json
HTTP 429 Too Many Requests
{
  "detail": "Rate limit exceeded: 20 requests per minute for this endpoint"
}
```

### Invalid Input
```json
HTTP 422 Unprocessable Entity
{
  "detail": [
    {
      "loc": ["body", "query_text"],
      "msg": "Query too short (minimum 3 characters)",
      "type": "value_error"
    }
  ]
}
```

### ChromaDB Failure
```json
HTTP 500 Internal Server Error
{
  "detail": "ChromaDB query failed: database not initialized",
  "suggestion": "Run 'python cli.py ingest-all' to initialize"
}
```

---

## Streamlit UI Walkthrough

### Chat Tab
1. User types: "What was NREGA budget in 2010?"
2. System auto-detects policy: NREGA
3. Shows answer with evidence cards
4. Click "ðŸ“Š Evidence" expander to see:
   - Source year
   - Modality (budget/temporal/news)
   - Similarity score
   - Time-decay weight
   - Access count

### Drift Tab
1. Select policy: NREGA
2. Select modality: temporal
3. Click "ðŸ” Analyze"
4. View:
   - Max drift period (2019â†’2020, score 0.74)
   - Timeline with color-coded severity
   - Year-by-year drift scores

### Upload Tab
1. Upload MP3 file (voice query)
2. System transcribes: "Tell me about farmer schemes"
3. Auto-detects policy: PM-KISAN
4. User can override if wrong
5. Click "âš¡ INGEST" to add to database

### Recommendations Tab
1. Select policy: NREGA
2. Set count: 5
3. Click "ðŸ” Get Recommendations"
4. View related policies sorted by similarity
5. System shows actionable recommendations:
   - HIGH similarity (>0.85): Check for conflicts
   - MEDIUM (0.70-0.85): Look for complementary provisions
   - LOW (<0.70): Consider for best practices

---

## Test Dataset Details

Our evaluation uses 20 queries across 2 policies:

**NREGA (10 queries):**
- 3 budget queries (e.g., "What was budget in 2020?")
- 4 evolution queries (e.g., "How did it change in 2009?")
- 2 discourse queries (e.g., "What was public opinion in 2023?")
- 1 intent query ("What was original intent in 2005?")

**RTI (10 queries):**
- Similar distribution

**Ground truth:** Manually created by us based on known facts. Not independently validated.

**Results:**
- Year accuracy: 100% (for year-specific queries)
- Modality accuracy: 60% (budget vs temporal often confused)
- Average top-1 similarity: 0.575

**What 60% modality accuracy means:**
If you ask "What happened to NREGA in 2014?", you'll get 2014 data (correct year), but might get budget allocation when you wanted policy evolution text. The answer is still useful, just not the exact type you expected.

---

These examples are from actual system runs. No cherry-picking. We included failures (OCR gibberish, low-confidence queries) to show realistic behavior.
